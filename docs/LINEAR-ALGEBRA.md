# ğŸ“ Linear Algebra for Machine Learning

## Overview

Linear algebra is the mathematical foundation of machine learning. Understanding it deeply improves model design and debugging.

---

## ğŸ—ï¸ Fundamental Concepts

### Vectors
- **Definition**: Ordered collection of numbers
- **Notation**: Column vector **v** or row vector **v**áµ€
- **Operations**: Addition, scalar multiplication, dot product
- **Norms**: L1, L2, Lâˆ norms
- **Properties**: Magnitude, direction, orthogonality

### Matrices
- **Definition**: Rectangular array of numbers
- **Shape**: (m Ã— n) for m rows, n columns
- **Operations**: Addition, multiplication, transpose, inverse
- **Determinant**: Measure of invertibility
- **Trace**: Sum of diagonal elements

### Tensors
- **0D**: Scalar (number)
- **1D**: Vector (list)
- **2D**: Matrix (table)
- **3D+**: Higher-order tensors
- **Operations**: Element-wise, contractions, outer products

---

## ğŸ“Š Matrix Decompositions

### Singular Value Decomposition (SVD)
**Formula**: A = UÎ£Váµ€
- **U**: Left singular vectors
- **Î£**: Singular values (diagonal)
- **V**: Right singular vectors
- **Applications**: Dimensionality reduction, image compression, low-rank approximation
- **Properties**: Always exists, works for rectangular matrices

### Eigenvalue Decomposition
**Formula**: A = PDPâ»Â¹
- **P**: Matrix of eigenvectors
- **D**: Diagonal matrix of eigenvalues
- **Condition**: Works for square matrices
- **Applications**: PCA, power iteration, stability analysis
- **Properties**: Real eigenvalues for symmetric matrices

### QR Decomposition
**Formula**: A = QR
- **Q**: Orthogonal matrix (Qáµ€Q = I)
- **R**: Upper triangular matrix
- **Applications**: Solving linear systems, least squares, numerical stability
- **Stability**: More numerically stable than normal equations

### Cholesky Decomposition
**Formula**: A = LLáµ€ (for positive definite A)
- **L**: Lower triangular matrix
- **Conditions**: A must be symmetric positive definite
- **Applications**: Solving linear systems, simulation, optimization
- **Efficiency**: 2x faster than LU decomposition

### LU Decomposition
**Formula**: A = LU
- **L**: Lower triangular (with 1s on diagonal)
- **U**: Upper triangular
- **Applications**: Solving linear systems, computing determinants
- **Efficiency**: O(nÂ³) computation

### Polar Decomposition
**Formula**: A = UP
- **U**: Unitary matrix (preserves lengths)
- **P**: Positive semidefinite matrix
- **Applications**: Rigid transformations, orientation extraction

---

## ğŸ” Spectral Analysis

### Eigenvalues & Eigenvectors
- **Definition**: Av = Î»v (v is eigenvector, Î» is eigenvalue)
- **Geometric**: Direction that doesn't change under transformation
- **Computation**: Solve det(A - Î»I) = 0
- **Applications**: Stability analysis, vibration analysis, PageRank

### Spectral Theorem
- For symmetric matrix A: A = PDPâ»Â¹ where P is orthogonal
- **Corollary**: Symmetric matrices have real eigenvalues & orthogonal eigenvectors
- **Applications**: Optimization, quadratic forms

### Power Iteration
- Find largest eigenvalue iteratively
- **Algorithm**: x_{n+1} = Ax_n / ||Ax_n||
- **Convergence**: Exponential
- **Applications**: PageRank, recommendation systems

### Condition Number
- **Îº(A) = Ïƒ_max / Ïƒ_min** (ratio of largest to smallest singular value)
- **Interpretation**: Sensitivity to input perturbations
- **Well-conditioned**: Îº(A) â‰ˆ 1
- **Ill-conditioned**: Îº(A) is large (numerical instability)

---

## ğŸ¯ Norms & Distances

### Vector Norms
- **L1 Norm**: ||v||â‚ = Î£|váµ¢| (Manhattan distance)
- **L2 Norm**: ||v||â‚‚ = âˆš(Î£váµ¢Â²) (Euclidean distance)
- **Lâˆ Norm**: ||v||âˆ = max|váµ¢| (Chebyshev distance)
- **Lp Norm**: ||v||_p = (Î£|váµ¢|^p)^(1/p)

### Matrix Norms
- **Frobenius**: ||A||_F = âˆš(Î£áµ¢â±¼ aáµ¢â±¼Â²)
- **Spectral**: ||A||â‚‚ = Ïƒ_max(A)
- **Nuclear**: ||A||â‚Š = Î£ Ïƒáµ¢ (sum of singular values)

### Distance Metrics
- **Euclidean**: âˆš(Î£(xáµ¢ - yáµ¢)Â²)
- **Manhattan**: Î£|xáµ¢ - yáµ¢|
- **Cosine**: 1 - (xÂ·y)/(||x||||y||)
- **Mahalanobis**: âˆš((x-Î¼)áµ€Î£â»Â¹(x-Î¼))

---

## ğŸ§® Optimization & Calculus

### Gradients & Jacobians
- **Gradient**: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]
- **Jacobian**: Matrix of all first-order partial derivatives
- **Hessian**: Matrix of second-order partial derivatives
- **Chain Rule**: For composite functions

### Convexity
- **Convex Function**: f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)
- **Convex Set**: All points between any two points are in set
- **Convex Optimization**: Global optimum guaranteed
- **Applications**: Regression, SVM, deep learning loss

### Quadratic Forms
- **Definition**: x^T A x (where A is symmetric)
- **Positive Definite**: x^T A x > 0 for all x â‰  0
- **Applications**: Covariance matrices, regularization terms
- **Properties**: Eigenvalues determine definiteness

### Matrix Calculus Rules
- âˆ‡_X tr(AX) = A^T
- âˆ‡_X tr(X^T A X) = 2AX
- âˆ‡_X ||AX - b||Â² = 2A^T(AX - b)
- âˆ‡_X log det(X) = (X^T)^(-1)

---

## ğŸ“ˆ Linear Systems & Solutions

### Solving Ax = b
1. **Square matrices (nÃ—n)**: x = Aâ»Â¹b (if invertible)
2. **Overdetermined (m>n)**: Least squares x = (A^T A)â»Â¹ A^T b
3. **Underdetermined (m<n)**: Minimum norm solution
4. **Singular**: Use pseudo-inverse Aâº

### Pseudo-Inverse (Moore-Penrose)
- **Definition**: Aâº = V Î£âº U^T (from SVD)
- **Properties**: Always exists, generalizes inverse
- **Least Squares**: Minimizes ||Ax - b||Â²
- **Minimum Norm**: Among solutions with minimum norm

### Regularization
- **Ridge (L2)**: (A^T A + Î»I)â»Â¹ A^T b
- **Lasso (L1)**: Sparse solutions via optimization
- **Elastic Net**: Combination of L1 & L2

---

## ğŸ“ Applications in ML

### Principal Component Analysis (PCA)
- Eigendecomposition of covariance matrix
- Find directions of maximum variance
- Dimensionality reduction

### Singular Value Decomposition Applications
- **SVD for recommendations**: User-item matrix factorization
- **Image compression**: Keep top k singular values
- **Noise reduction**: Remove small singular values

### Kernel Methods
- **Kernel Matrix**: K_ij = k(x_i, x_j)
- **Gram Matrix**: X X^T
- **Properties**: Symmetric, positive semi-definite
- **Applications**: SVM, kernel ridge regression, Gaussian processes

### Neural Networks
- **Weight matrices**: Feature transformation
- **Initialization**: Random matrices with specific distributions
- **Backpropagation**: Chain rule through matrix multiplications

---

## ğŸš€ Computational Considerations

### Numerical Stability
- Use QR decomposition instead of normal equations
- Avoid computing inverse explicitly
- Use stable algorithms for eigendecomposition

### Computational Complexity
- **Matrix multiplication**: O(nÂ³) for dense
- **SVD**: O(mnÂ²) for mâ‰¥n
- **Eigendecomposition**: O(nÂ³)
- **Sparse operations**: Much faster for sparse matrices

### Libraries
- **NumPy**: Dense linear algebra
- **SciPy**: Advanced operations
- **scikit-learn**: ML-focused
- **CuPy**: GPU-accelerated (CUDA)
- **JAX**: Automatic differentiation + linear algebra

---

## ğŸ“š Important Theorems

- **Spectral Theorem**: Symmetric matrices diagonalizable
- **Singular Value Theorem**: Every matrix has SVD
- **Rank-Nullity**: rank(A) + nullity(A) = n
- **Cayley-Hamilton**: Every matrix satisfies its characteristic equation

---

*Detailed implementations and applications in projects folder.*
